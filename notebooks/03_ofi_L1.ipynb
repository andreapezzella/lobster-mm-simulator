{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR = Path(\"..\") / \"Data\"\n",
    "PROCESSED_DIR = Path(\"..\") / \"data\" / \"processed\"\n",
    "\n",
    "PRICE_SCALE = 10_000.0\n",
    "LEVELS = 10\n",
    "ASK_DUMMY = 9_999_999_999\n",
    "BID_DUMMY = -9_999_999_999\n",
    "\n",
    "MSG_PATH = RAW_DIR / \"GOOG_2012-06-21_34200000_57600000_message_10.csv\"\n",
    "OB_PATH  = RAW_DIR / \"GOOG_2012-06-21_34200000_57600000_orderbook_10.csv\"\n",
    "\n",
    "PARQUET_PATH = PROCESSED_DIR / \"goog_2012-06-21.parquet\"\n",
    "PICKLE_PATH  = PROCESSED_DIR / \"goog_2012-06-21.pkl\"\n",
    "\n",
    "\n",
    "def load_df_standalone(levels: int = LEVELS) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standalone loader:\n",
    "    - if processed parquet/pkl exists -> load it\n",
    "    - else -> read raw CSVs, replace dummy prices & set time index\n",
    "    \"\"\"\n",
    "    if PARQUET_PATH.exists():\n",
    "        return pd.read_parquet(PARQUET_PATH)\n",
    "    if PICKLE_PATH.exists():\n",
    "        return pd.read_pickle(PICKLE_PATH)\n",
    "\n",
    "    msg_cols = [\"time\", \"type\", \"order_id\", \"size\", \"price\", \"direction\"]\n",
    "    msg = pd.read_csv(MSG_PATH, header=None, names=msg_cols)\n",
    "\n",
    "    ob_cols = []\n",
    "    for lvl in range(1, levels + 1):\n",
    "        ob_cols += [f\"ask_p{lvl}\", f\"ask_s{lvl}\", f\"bid_p{lvl}\", f\"bid_s{lvl}\"]\n",
    "    ob = pd.read_csv(OB_PATH, header=None, names=ob_cols, usecols=list(range(4 * levels)))\n",
    "\n",
    "    assert len(msg) == len(ob), f\"Row mismatch: msg={len(msg)} vs ob={len(ob)}\"\n",
    "\n",
    "    df = pd.concat([msg, ob], axis=1).sort_values(\"time\", kind=\"mergesort\")\n",
    "    df = df.set_index(\"time\", drop=False)\n",
    "\n",
    "    ask_price_cols = [f\"ask_p{i}\" for i in range(1, levels + 1)]\n",
    "    bid_price_cols = [f\"bid_p{i}\" for i in range(1, levels + 1)]\n",
    "    df[ask_price_cols] = df[ask_price_cols].replace(ASK_DUMMY, np.nan)\n",
    "    df[bid_price_cols] = df[bid_price_cols].replace(BID_DUMMY, np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_df_standalone()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-of-book validity\n",
    "valid_top = df[\"ask_p1\"].notna() & df[\"bid_p1\"].notna()\n",
    "\n",
    "# Mid & spread\n",
    "df[\"mid_d\"] = np.nan\n",
    "df.loc[valid_top, \"mid_d\"] = (df.loc[valid_top, \"ask_p1\"] + df.loc[valid_top, \"bid_p1\"]) / (2 * PRICE_SCALE)\n",
    "\n",
    "df[\"spread_d\"] = np.nan\n",
    "df.loc[valid_top, \"spread_d\"] = (df.loc[valid_top, \"ask_p1\"] - df.loc[valid_top, \"bid_p1\"]) / PRICE_SCALE\n",
    "\n",
    "# Next-event mid change\n",
    "df[\"dmid_1\"] = df[\"mid_d\"].shift(-1) - df[\"mid_d\"]\n",
    "\n",
    "# FI L1 (event-based)\n",
    "bp, bs = df[\"bid_p1\"], df[\"bid_s1\"]\n",
    "ap, aS = df[\"ask_p1\"], df[\"ask_s1\"]\n",
    "\n",
    "bp_prev, bs_prev = bp.shift(1), bs.shift(1)\n",
    "ap_prev, aS_prev = ap.shift(1), aS.shift(1)\n",
    "\n",
    "bid_contrib = np.where(bp > bp_prev, bs,\n",
    "               np.where(bp < bp_prev, -bs_prev,\n",
    "               (bs - bs_prev)))\n",
    "\n",
    "ask_contrib = np.where(ap < ap_prev, aS,\n",
    "               np.where(ap > ap_prev, -aS_prev,\n",
    "               (aS - aS_prev)))\n",
    "\n",
    "df[\"ofi_l1\"] = bid_contrib - ask_contrib\n",
    "depth_l1 = (df[\"bid_s1\"] + df[\"ask_s1\"]).replace(0, np.nan)\n",
    "df[\"ofi_l1_norm\"] = df[\"ofi_l1\"] / depth_l1\n",
    "\n",
    "\n",
    "# Analysis table\n",
    "tmp = df.loc[valid_top, [\"ofi_l1\", \"ofi_l1_norm\", \"dmid_1\", \"spread_d\"]].dropna()\n",
    "\n",
    "# Sanity checks\n",
    "neg_share = (tmp[\"spread_d\"] < 0).mean()\n",
    "assert neg_share < 1e-6, f\"Negative spread share too high: {neg_share:.2e}\"\n",
    "assert (tmp[\"spread_d\"] >= 0).mean() > 0.999999\n",
    "print(\"Rows used:\", len(tmp))\n",
    "tmp[[\"ofi_l1\", \"dmid_1\", \"spread_d\"]].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8701c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200_000\n",
    "s = tmp.head(N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(s[\"ofi_l1\"], s[\"dmid_1\"], s=2, alpha=0.2)\n",
    "ax.set_title(\"Next mid change vs OFI L1\")\n",
    "ax.set_xlabel(\"ofi_l1 (shares)\")\n",
    "ax.set_ylabel(\"delta_mid (t+1) ($)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../reports/figures/03_ofi_vs_dmid_scatter.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Corr(ofi_l1, dmid_1):\", s[\"ofi_l1\"].corr(s[\"dmid_1\"]))\n",
    "\n",
    "# Ofi normalized\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(tmp.index[:N], tmp[\"ofi_l1_norm\"].iloc[:N], linewidth=1)\n",
    "ax.set_title(\"Normalized OFI L1 â€” N events\")\n",
    "ax.set_xlabel(\"time (seconds)\")\n",
    "ax.set_ylabel(\"ofi / depth\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../reports/figures/03_ofi_norm_firstN.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009804d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anna5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
